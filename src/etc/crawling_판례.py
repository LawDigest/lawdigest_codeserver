import pandas as pd
import os
import time
import re
from xml.etree import ElementTree
import json
from tqdm.asyncio import tqdm
from bs4 import BeautifulSoup
from urllib.parse import urljoin, parse_qs, urlparse
import io
import asyncio
import aiohttp
import aiofiles
import aiofiles.os as aio_os

# --- PDF ì¶”ì¶œì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
# ì‹¤í–‰ ì „ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install pdfplumber
import pdfplumber

# --- ìƒìˆ˜ ì •ì˜ ---
DEFAULT_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Connection': 'keep-alive',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'
}
BASE_URL_SEARCH = "http://www.law.go.kr/DRF/lawSearch.do"
BASE_URL_SERVICE = "http://www.law.go.kr/DRF/lawService.do"
NTS_PDF_DOWNLOAD_URL = "https://taxlaw.nts.go.kr/downloadStorFile.do"
LAWGO_PDF_DOWNLOAD_URL = "https://www.law.go.kr/LSW/precPdfPrint.do"
BASE_OUTPUT_DIR = os.path.join("data", "raw")


class LawGovKrScraper:
    """
    ëŒ€í•œë¯¼êµ­ ë²•ì œì²˜ êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°ì˜ íŒë¡€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ë¹„ë™ê¸° ìŠ¤í¬ë ˆì´í¼ í´ë˜ìŠ¤.
    [ê°œì„ ] ë³´ê³ ì„œ ì§‘ê³„ ë¡œì§ ìˆ˜ì •.
    """

    def __init__(self, oc_id, request_delay=0.2, max_retries=3):
        if not oc_id:
            raise ValueError("OC ID (ì¸ì¦í‚¤)ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
        self.oc_id = oc_id
        self.request_delay = request_delay
        self.max_retries = max_retries
        os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)

    async def _make_request(self, session, method, url, params=None, data=None, headers=None, allow_redirects=True):
        """ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¹„ë™ê¸° ìš”ì²­ ì²˜ë¦¬ ë©”ì„œë“œ (ë°ì´í„° ì½ê¸°ê¹Œì§€ ì¬ì‹œë„ ë° ë¡œê·¸ ì •ì œ)"""
        await asyncio.sleep(self.request_delay)
        last_exception = None
        req_headers = headers or session.headers
        
        for attempt in range(self.max_retries):
            try:
                async with session.request(method, url, params=params, data=data, headers=req_headers,
                                           allow_redirects=allow_redirects, timeout=30) as response:
                    response.raise_for_status()
                    content = await response.read()
                    return content, response.url
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                last_exception = e
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(1)
                else:
                    tqdm.write(f"âŒ ìµœëŒ€ ì¬ì‹œë„({self.max_retries}íšŒ) í›„ ìš”ì²­ ìµœì¢… ì‹¤íŒ¨. URL: {url}, ì˜¤ë¥˜: {last_exception}")
        return None, None

    async def _parse_list_response(self, response_content):
        """íŒë¡€ ëª©ë¡ APIì˜ XML ì‘ë‹µì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        if not response_content: return [], 0
        try:
            root = ElementTree.fromstring(response_content)
            data = [{child.tag: child.text for child in item} for item in root.findall('prec')]
            total_count_element = root.find('totalCnt')
            total_count = int(total_count_element.text) if total_count_element is not None else 0
            return data, total_count
        except Exception as e:
            tqdm.write(f"  âŒ XML íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return [], 0

    async def fetch_case_list(self, session, query=None, date=None, date_range=None, display=100, max_pages=None):
        """ê²€ìƒ‰ì–´ ë˜ëŠ” íŒë¡€ ì„ ê³ ì¼ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒë¡€ ëª©ë¡ì„ ë¹„ë™ê¸°ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        if not query and not date and not date_range:
            print("âš ï¸ ê²€ìƒ‰ì–´(query), ì„ ê³ ì¼ì(date), ë˜ëŠ” ì„ ê³ ì¼ì ë²”ìœ„(date_range) ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
            return pd.DataFrame()

        search_desc = []
        if query: search_desc.append(f"ê²€ìƒ‰ì–´ '{query}'")
        if date: search_desc.append(f"ì„ ê³ ì¼ì '{date}'")
        if date_range: search_desc.append(f"ì„ ê³ ì¼ì ë²”ìœ„ '{date_range}'")
        
        print(f"â¡ï¸ {', '.join(search_desc)}ì— ëŒ€í•œ íŒë¡€ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘...")

        params = {'OC': self.oc_id, 'target': 'prec', 'type': 'xml', 'display': display, 'page': 1}
        if query: params['query'] = query
        if date: params['precJoYd'] = date.replace('.', '')
        if date_range: params['prncYd'] = date_range

        content, _ = await self._make_request(session, 'GET', BASE_URL_SEARCH, params=params)
        if not content: return pd.DataFrame()

        initial_data, total_count = await self._parse_list_response(content)
        if total_count == 0:
            print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        all_data = initial_data
        total_pages = (total_count + display - 1) // display
        pages_to_fetch = min(max_pages, total_pages) if max_pages is not None else total_pages
        print(f"ğŸ“Š ì´ {total_count}ê°œì˜ íŒë¡€ ë°œê²¬. {pages_to_fetch}í˜ì´ì§€ì— ê±¸ì³ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

        if pages_to_fetch > 1:
            tasks = [self._make_request(session, 'GET', BASE_URL_SEARCH, params={**params, 'page': i}) for i in range(2, pages_to_fetch + 1)]
            for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="ğŸ“– íŒë¡€ ëª©ë¡ ìˆ˜ì§‘ ì¤‘"):
                page_content, _ = await f
                if page_content:
                    data, _ = await self._parse_list_response(page_content)
                    if data: all_data.extend(data)
        
        df = pd.DataFrame(all_data)
        print(f"\nğŸ‰ íŒë¡€ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df)}ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return df

    async def _extract_text_from_pdf(self, pdf_content):
        """PDF ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)"""
        if not pdf_content: return None
        last_exception = None
        for attempt in range(self.max_retries):
            try:
                with io.BytesIO(pdf_content) as pdf_file, pdfplumber.open(pdf_file) as pdf:
                    text = "\n".join(page.extract_text() for page in pdf.pages if page.extract_text())
                    if text: return text
                    last_exception = ValueError("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ë‚´ìš©ì´ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ).")
            except Exception as e:
                last_exception = e
            if attempt < self.max_retries - 1: await asyncio.sleep(1)
        tqdm.write(f"âŒ PDF íŒŒì‹± ìµœì¢… ì‹¤íŒ¨ (ì‹œë„ {self.max_retries}íšŒ). ì˜¤ë¥˜: {last_exception}")
        return None
    
    def _sanitize_filename(self, filename):
        return re.sub(r'[\\/*?:"<>|]', "", str(filename)).strip() if filename else ""

    async def _download_pdf_from_nts(self, session, final_page_url, case_row):
        parsed_url = urlparse(str(final_page_url))
        query_params = parse_qs(parsed_url.query)
        ntst_dcm_id = query_params.get('ntstDcmId', [None])[0]
        if not ntst_dcm_id: return None
        
        post_data = {"data": json.dumps({"dcmDVO": {"ntstDcmId": ntst_dcm_id}}), "actionId": "ASIQTB002PR02", "fileType": "pdf", "fileName": case_row['ì‚¬ê±´ëª…']}
        post_headers = {"Content-Type": "application/x-www-form-urlencoded", "Referer": str(final_page_url), "Origin": f"{parsed_url.scheme}://{parsed_url.netloc}"}
        pdf_content, _ = await self._make_request(session, 'POST', NTS_PDF_DOWNLOAD_URL, data=post_data, headers=post_headers)
        return pdf_content

    async def _download_pdf_from_lawgo(self, session, final_page_url, case_row):
        post_data = {"precSeq": case_row['íŒë¡€ì¼ë ¨ë²ˆí˜¸'], "fileType": "pdf", "preview": "N", "conJo": "1,2,3,4,5"}
        post_headers = {"Content-Type": "application/x-www-form-urlencoded", "Referer": str(final_page_url), "Origin": "https://www.law.go.kr"}
        pdf_content, _ = await self._make_request(session, 'POST', LAWGO_PDF_DOWNLOAD_URL, data=post_data, headers=post_headers)
        return pdf_content

    async def _fetch_and_save_case(self, session, case_row, period_output_dir):
        case_id = case_row['íŒë¡€ì¼ë ¨ë²ˆí˜¸']
        params = {'OC': self.oc_id, 'target': 'prec', 'type': 'HTML', 'ID': case_id}
        
        shell_content, shell_url = await self._make_request(session, 'GET', BASE_URL_SERVICE, params=params)
        if not shell_content: return {'status': 'DOWNLOAD_FAIL', 'case_info': case_row}

        soup = BeautifulSoup(shell_content, 'html.parser')
        url_input = soup.find('input', {'type': 'hidden', 'id': 'url'})
        lsw_url_path = url_input['value'] if url_input and 'value' in url_input.attrs else None
        if not lsw_url_path:
            iframe = soup.find('iframe')
            lsw_url_path = iframe['src'] if iframe and 'src' in iframe.attrs else None

        if not lsw_url_path: return {'status': 'DOWNLOAD_FAIL', 'case_info': case_row}
        lsw_url = urljoin(str(shell_url), lsw_url_path)

        lsw_content, final_page_url = await self._make_request(session, 'GET', lsw_url, headers={'Referer': str(shell_url)})
        if not lsw_content: return {'status': 'DOWNLOAD_FAIL', 'case_info': case_row}

        pdf_bytes = await self._download_pdf_from_nts(session, final_page_url, case_row) if "taxlaw.nts.go.kr" in str(final_page_url) else await self._download_pdf_from_lawgo(session, final_page_url, case_row)
        if not pdf_bytes: return {'status': 'DOWNLOAD_FAIL', 'case_info': case_row}
        
        text = await self._extract_text_from_pdf(pdf_bytes)
        if not text: return {'status': 'PARSE_FAIL', 'case_info': case_row}

        base_filename = f"{case_row['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}_{self._sanitize_filename(case_row['ì‚¬ê±´ë²ˆí˜¸'])}"
        pdf_path = os.path.join(period_output_dir, f"{base_filename}.pdf")
        txt_path = os.path.join(period_output_dir, f"{base_filename}.txt")
        try:
            async with aiofiles.open(pdf_path, 'wb') as f: await f.write(pdf_bytes)
            async with aiofiles.open(txt_path, 'w', encoding='utf-8') as f: await f.write(text)
        except Exception as e:
            tqdm.write(f"  [ì‹¤íŒ¨] ID {case_id}: íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")
            return {'status': 'SAVE_FAIL', 'case_info': case_row}
        
        return {'status': 'SUCCESS', 'text': text, 'case_info': case_row}

    async def _process_cases_batch(self, session, case_df, period_output_dir):
        """ì£¼ì–´ì§„ ë°ì´í„°í”„ë ˆì„ì˜ íŒë¡€ë“¤ì„ ìˆ˜ì§‘í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not isinstance(case_df, pd.DataFrame) or case_df.empty: return pd.DataFrame(), {}

        print(f"â¡ï¸ {len(case_df)}ê°œ íŒë¡€ì˜ ë³¸ë¬¸ ìˆ˜ì§‘ ë° ì €ì¥ ì‹œì‘...")
        
        tasks = [self._fetch_and_save_case(session, row, period_output_dir) for _, row in case_df.iterrows()]
        results = await tqdm.gather(*tasks, desc="âœï¸  íŒë¡€ ë³¸ë¬¸ ìˆ˜ì§‘/ì €ì¥ ì¤‘")

        status_map = {'SUCCESS': [], 'DOWNLOAD_FAIL': [], 'PARSE_FAIL': [], 'SAVE_FAIL': []}
        all_texts = []
        for res in results:
            # SKIPPED_EXISTSëŠ” ì´ í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ê¸°ë³¸ê°’ìœ¼ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‘¡ë‹ˆë‹¤.
            status_map.setdefault(res['status'], []).append(res['case_info'])
            all_texts.append(res.get('text'))
        
        df_with_texts = case_df.copy()
        df_with_texts['íŒë¡€ë³¸ë¬¸'] = all_texts
        
        return df_with_texts, status_map

    async def _write_period_summary(self, period_output_dir, filename_suffix, status_map):
        """ìˆ˜ì§‘ ê¸°ê°„ì— ëŒ€í•œ ìš”ì•½ ë¦¬í¬íŠ¸ íŒŒì¼ì„ ì‘ì„±í•©ë‹ˆë‹¤."""
        success_list = status_map.get('SUCCESS', [])
        skipped_list = status_map.get('SKIPPED_EXISTS', [])
        download_fail_list = status_map.get('DOWNLOAD_FAIL', [])
        parse_fail_list = status_map.get('PARSE_FAIL', [])
        save_fail_list = status_map.get('SAVE_FAIL', [])
        
        total_cases = sum(len(v) for v in status_map.values())
        newly_success_count = len(success_list)
        total_success_count = newly_success_count + len(skipped_list)
        success_rate = (total_success_count / total_cases * 100) if total_cases > 0 else 0
        
        summary_path = os.path.join(period_output_dir, f"crawling_result_{filename_suffix}.txt")
        async with aiofiles.open(summary_path, 'w', encoding='utf-8') as f:
            await f.write(f"--- ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ({filename_suffix}) ---\n")
            await f.write(f"ì´ ëŒ€ìƒ: {total_cases}ê±´\n")
            await f.write(f"ìµœì¢… ì„±ê³µ: {total_success_count}ê±´ (ì‹ ê·œ ìˆ˜ì§‘: {newly_success_count}ê±´, ê±´ë„ˆë›°ê¸°: {len(skipped_list)}ê±´)\n")
            await f.write(f"ìˆ˜ì§‘ ì‹¤íŒ¨: {total_cases - total_success_count}ê±´\n")
            await f.write(f"ì„±ê³µë¥ : {success_rate:.2f}%\n")
            await f.write("\n--- ì„¸ë¶€ ë‚´ì—­ ---\n")
            
            if download_fail_list:
                await f.write(f"\n[ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {len(download_fail_list)}ê±´]\n")
                for item in download_fail_list: await f.write(f"- ID: {item['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}, ì‚¬ê±´ë²ˆí˜¸: {item['ì‚¬ê±´ë²ˆí˜¸']}\n")
            if parse_fail_list:
                await f.write(f"\n[PDF íŒŒì‹± ì‹¤íŒ¨: {len(parse_fail_list)}ê±´]\n")
                for item in parse_fail_list: await f.write(f"- ID: {item['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}, ì‚¬ê±´ë²ˆí˜¸: {item['ì‚¬ê±´ë²ˆí˜¸']}\n")
            if save_fail_list:
                await f.write(f"\n[íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {len(save_fail_list)}ê±´]\n")
                for item in save_fail_list: await f.write(f"- ID: {item['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}, ì‚¬ê±´ë²ˆí˜¸: {item['ì‚¬ê±´ë²ˆí˜¸']}\n")
        print(f"ğŸ“„ ê¸°ê°„ë³„ ë¦¬í¬íŠ¸ê°€ '{summary_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def generate_filename_suffix(**kwargs):
    parts = [f"{k}_{v.replace('.', '-').replace('~', '-')}" for k, v in kwargs.items() if v]
    return "_".join(re.sub(r'[\\/*?:"<>|~]', "_", part) for part in parts)

async def main():
    MY_OC_ID = "leegy76"
    if MY_OC_ID == "YOUR_OC_ID_HERE":
        print("ğŸš¨ ì½”ë“œ ì‹¤í–‰ ì „ì— 'MY_OC_ID' ë³€ìˆ˜ì— ë°œê¸‰ë°›ì€ ì¸ì¦í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    async def _write_overall_summary(stats_list):
        """ì „ì²´ ìˆ˜ì§‘ í˜„í™©ì„ íŒŒì¼ì— ê¸°ë¡í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        print("\n" + "="*60 + "\nğŸ”„ï¸ ì¢…í•© ìˆ˜ì§‘ í˜„í™© ì—…ë°ì´íŠ¸...\n" + "="*60)
        successful_periods = [s for s in stats_list if s['status'] == 'SUCCESS']
        nodata_periods = [s for s in stats_list if s['status'] == 'NO_DATA']
        failed_periods = [s for s in stats_list if s['status'] == 'FATAL_ERROR']
        
        total_periods = len(stats_list)
        success_rate = (len(successful_periods) + len(nodata_periods)) / total_periods * 100 if total_periods > 0 else 0

        summary_text = [
            f"ì´ ì‹œë„ ê¸°ê°„: {total_periods}ê°œ",
            f"ìˆ˜ì§‘ ì„±ê³µ (ë°ì´í„° ìˆìŒ): {len(successful_periods)}ê°œ",
            f"ìˆ˜ì§‘ ì„±ê³µ (ë°ì´í„° ì—†ìŒ): {len(nodata_periods)}ê°œ",
            f"ìˆ˜ì§‘ ì‹¤íŒ¨: {len(failed_periods)}ê°œ",
            f"ì„±ê³µë¥ : {success_rate:.2f}%",
            "\n--- ê¸°ê°„ë³„ ìƒì„¸ í˜„í™© ---"
        ]
        for stat in stats_list:
            period = stat['period']
            status = stat['status']
            if status == 'SUCCESS':
                s_map = stat['stats']
                total = sum(len(v) for v in s_map.values())
                success_count = len(s_map.get('SUCCESS', [])) + len(s_map.get('SKIPPED_EXISTS', []))
                summary_text.append(f"- {period}: ì„±ê³µ (ì²˜ë¦¬ {success_count}/{total}ê±´)")
            else:
                summary_text.append(f"- {period}: {status}")

        report_str = "\n".join(summary_text)
        print(report_str)
        
        report_path = os.path.join(BASE_OUTPUT_DIR, "crawling_result_overall.txt")
        async with aiofiles.open(report_path, 'w', encoding='utf-8') as f:
            await f.write(report_str)
        print(f"\nğŸ’¾ ì¢…í•© ë¦¬í¬íŠ¸ê°€ '{report_path}'ì— ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

    async with aiohttp.ClientSession(headers=DEFAULT_HEADERS) as session:
        scraper = LawGovKrScraper(oc_id=MY_OC_ID)

        async def run_collection(params, is_test=False):
            """ë‹¨ì¼ ìˆ˜ì§‘ ì‘ì—…ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
            filename_suffix = generate_filename_suffix(**params)
            period_output_dir = os.path.join(BASE_OUTPUT_DIR, filename_suffix)
            os.makedirs(period_output_dir, exist_ok=True)
            
            print(f"\n" + "="*50 + f"\nâ–¶ï¸ {'í…ŒìŠ¤íŠ¸' if is_test else 'ì „ì²´'} ìˆ˜ì§‘ ì‹¤í–‰: {filename_suffix}\n" + "="*50)
            
            list_params = {'display': 5 if is_test else 100, 'max_pages': 1 if is_test else None}
            df_list = await scraper.fetch_case_list(session, **params, **list_params)

            if df_list is None or df_list.empty:
                print(f"\nâš ï¸ {filename_suffix} ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None

            print(f"\n[ê²€ì¦] ê¸°ì¡´ì— ìˆ˜ì§‘ëœ íŒŒì¼ì„ í™•ì¸í•˜ì—¬ ëˆ„ë½ëœ í•­ëª©ë§Œ ì„ ë³„í•©ë‹ˆë‹¤...")
            rows_to_collect = []
            rows_to_skip = []
            
            for _, row in df_list.iterrows():
                base_filename = f"{row['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}_{scraper._sanitize_filename(row['ì‚¬ê±´ë²ˆí˜¸'])}"
                pdf_path = os.path.join(period_output_dir, f"{base_filename}.pdf")
                txt_path = os.path.join(period_output_dir, f"{base_filename}.txt")
                if await aio_os.path.exists(pdf_path) and await aio_os.path.exists(txt_path):
                    rows_to_skip.append(row)
                else:
                    rows_to_collect.append(row)
            
            status_map = {'SKIPPED_EXISTS': [dict(row) for row in rows_to_skip]}
            df_final_collected = pd.DataFrame()

            if rows_to_collect:
                df_to_collect = pd.DataFrame(rows_to_collect).reset_index(drop=True)
                df_final_collected, collected_status_map = await scraper._process_cases_batch(session, df_to_collect, period_output_dir)
                for key, value in collected_status_map.items():
                    status_map.setdefault(key, []).extend(value)
            else:
                print("  âœ… ëª¨ë“  í•­ëª©ì´ ì´ë¯¸ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹ ê·œ ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

            await scraper._write_period_summary(period_output_dir, filename_suffix, status_map)

            print(f"\n[í†µí•©] ê¸°ì¡´ íŒŒì¼ê³¼ ìƒˆë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ í†µí•©í•©ë‹ˆë‹¤...")
            skipped_texts = []
            for row_dict in status_map['SKIPPED_EXISTS']:
                base_filename = f"{row_dict['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}_{scraper._sanitize_filename(row_dict['ì‚¬ê±´ë²ˆí˜¸'])}"
                txt_path = os.path.join(period_output_dir, f"{base_filename}.txt")
                try:
                    async with aiofiles.open(txt_path, 'r', encoding='utf-8') as f:
                        text = await f.read()
                    skipped_texts.append(text)
                except Exception:
                    skipped_texts.append(None)
            
            df_skipped = pd.DataFrame(status_map['SKIPPED_EXISTS'])
            if not df_skipped.empty:
                df_skipped['íŒë¡€ë³¸ë¬¸'] = skipped_texts
            
            df_final_total = pd.concat([df_final_collected, df_skipped], ignore_index=True)
            if not df_final_total.empty:
                df_final_total = df_final_total.set_index('íŒë¡€ì¼ë ¨ë²ˆí˜¸').loc[df_list['íŒë¡€ì¼ë ¨ë²ˆí˜¸'].astype(str)].reset_index()
                json_path = os.path.join(period_output_dir, f"collected_cases_{filename_suffix}.json")
                df_final_total.to_json(json_path, orient='records', force_ascii=False, indent=4)
                print(f"\nğŸ’¾ ìµœì¢… í†µí•© ê²°ê³¼(JSON)ê°€ '{json_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return status_map

        # # --- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ) ---
        # await run_collection({"query": "ì†í•´ë°°ìƒ"}, is_test=True)

        # --- ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ ---
        print("\n" + "="*60 + "\nâ–¶ï¸ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ (2000.01.01 ~ 2025.07.10)\n" + "="*60)
        start_date = pd.to_datetime("2000-01-01")
        end_date = pd.to_datetime("2025-07-10")
        
        date_ranges = pd.date_range(start=start_date, end=end_date, freq='MS')
        
        overall_stats = []
        collection_max_retries = 3

        for i in range(len(date_ranges)):
            range_start = date_ranges[i]
            range_end = (range_start + pd.offsets.MonthEnd(1)) if i < len(date_ranges) - 1 else end_date
            if range_end > end_date: range_end = end_date
            date_range_str = f"{range_start.strftime('%Y%m%d')}~{range_end.strftime('%Y%m%d')}"
            
            for attempt in range(collection_max_retries):
                try:
                    status_map = await run_collection({"date_range": date_range_str})
                    if status_map is not None:
                        overall_stats.append({'period': date_range_str, 'status': 'SUCCESS', 'stats': status_map})
                    else:
                         overall_stats.append({'period': date_range_str, 'status': 'NO_DATA', 'stats': {}})
                    break 
                except Exception as e:
                    print(f"â€¼ï¸ {date_range_str} ê¸°ê°„ ìˆ˜ì§‘ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt + 1}/{collection_max_retries}): {e}")
                    if attempt < collection_max_retries - 1:
                        await asyncio.sleep(1)
                    else:
                        print(f"â€¼ï¸ {date_range_str} ê¸°ê°„ ìˆ˜ì§‘ì— ìµœì¢… ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        overall_stats.append({'period': date_range_str, 'status': 'FATAL_ERROR', 'stats': {}})
            
            await _write_overall_summary(overall_stats)


if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
