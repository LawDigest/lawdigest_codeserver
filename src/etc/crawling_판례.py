"""
LawGovKrScraper

ì´ ëª¨ë“ˆì€ ëŒ€í•œë¯¼êµ­ ë²•ì œì²˜ êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°ì˜ íŒë¡€ ë°ì´í„°ë¥¼ ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  PDFë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- íŒë¡€ ëª©ë¡ ì¡°íšŒ ë° í˜ì´ì§•
- PDF ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
- íŒŒì¼ ê¸°ë°˜ ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€
- ì†ë„ ì œí•œ ë° í—ˆìš© ì‹œê°„ ì œì•½
- ë¡œê¹… ë° ë¦¬í¬íŠ¸ ìƒì„±
- í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì§€ì›
"""
import os
import re
import time
import datetime
import io
import json
import logging
import asyncio
from collections import deque
from xml.etree import ElementTree
from urllib.parse import urljoin, parse_qs, urlparse

import pandas as pd
import aiohttp
import aiofiles
import aiofiles.os as aio_os
from bs4 import BeautifulSoup
import pdfplumber
from tqdm import tqdm

# --- ì„¤ì • (Configuration) ---
# ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬. ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” 'data/raw'ì— ì €ì¥ë©ë‹ˆë‹¤.
BASE_OUTPUT_DIR = os.path.join("data", "raw")
# ì¬ìˆ˜ì§‘ ì‹œë„ ì‹œ ë°ì´í„°ê°€ ì €ì¥ë  ë””ë ‰í† ë¦¬ (í˜„ì¬ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
RECOLLECT_DIR = os.path.join(BASE_OUTPUT_DIR, "recollected_data")

# ê¸°ë³¸ HTTP ìš”ì²­ í—¤ë”
DEFAULT_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9',
    'Connection': 'keep-alive',
    'User-Agent': 'Mozilla/5.0' # ë´‡ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•´ ì¼ë°˜ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ ì„¤ì •
}
# ë²•ì œì²˜ API URL
BASE_URL_SEARCH = "http://www.law.go.kr/DRF/lawSearch.do" # ëª©ë¡ ê²€ìƒ‰ìš©
BASE_URL_SERVICE = "http://www.law.go.kr/DRF/lawService.do" # ìƒì„¸ ì •ë³´ìš©
# PDF ë‹¤ìš´ë¡œë“œ URL
NTS_PDF_DOWNLOAD_URL = "https://taxlaw.nts.go.kr/downloadStorFile.do" # êµ­ì„¸ì²­ íŒë¡€
LAWGO_PDF_DOWNLOAD_URL = "https://www.law.go.kr/LSW/precPdfPrint.do" # ë²•ì œì²˜ íŒë¡€

# --- ë¡œê¹… ì„¤ì • (Logging Setup) ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

class RateLimiter:
    """
    ë¹„ë™ê¸° í™˜ê²½ì—ì„œ API ìš”ì²­ ì†ë„ë¥¼ ì œì–´í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ ì‹œê°„(per_seconds) ë™ì•ˆ ìµœëŒ€ í˜¸ì¶œ íšŸìˆ˜(max_calls)ë¥¼ ì œí•œí•˜ì—¬ ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì…ë‹ˆë‹¤.

    ì‚¬ìš©ë²•:
        limiter = RateLimiter(max_calls=3, per_seconds=2)
        async with limiter:
            # ì´ ë¸”ë¡ ì•ˆì˜ ì½”ë“œëŠ” 2ì´ˆì— 3ë²ˆ ì´í•˜ë¡œë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
            await session.get(url)
    """
    def __init__(self, max_calls: int, per_seconds: float):
        self.max_calls = max_calls
        self.per_seconds = per_seconds
        self.calls = deque()

    async def __aenter__(self):
        """async with ì§„ì… ì‹œ í˜¸ì¶œ ì†ë„ë¥¼ ì²´í¬í•˜ê³  í•„ìš”ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤."""
        now = time.monotonic()
        # ì˜¤ë˜ëœ í˜¸ì¶œ ê¸°ë¡ ì œê±°
        while self.calls and now - self.calls[0] > self.per_seconds:
            self.calls.popleft()
        
        # ì œí•œ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ëŒ€ê¸°
        if len(self.calls) >= self.max_calls:
            sleep_time = self.per_seconds - (now - self.calls[0])
            if sleep_time > 0:
                await asyncio.sleep(sleep_time)
        
        self.calls.append(time.monotonic())

    async def __aexit__(self, exc_type, exc, tb):
        """async with ë¸”ë¡ì„ ë¹ ì ¸ë‚˜ê°ˆ ë•Œ ë³„ë„ ì‘ì—…ì€ ì—†ìŠµë‹ˆë‹¤."""
        pass

async def wait_for_window(start_hour=3, end_hour=9):
    """
    ì§€ì •ëœ ì‹œê°„ ì°½(ê¸°ë³¸: ìƒˆë²½ 3ì‹œ ~ ì˜¤ì „ 9ì‹œ)ì—ë§Œ ì‘ì—…ì´ ì‹¤í–‰ë˜ë„ë¡ ëŒ€ê¸°í•©ë‹ˆë‹¤.
    ì„œë²„ ë¶€í•˜ê°€ ì ì€ ì‹œê°„ëŒ€ì— í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
    
    í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ì´ í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    now = datetime.datetime.now()
    if not (start_hour <= now.hour < end_hour):
        target_time = now.replace(hour=start_hour, minute=0, second=0, microsecond=0)
        # í˜„ì¬ ì‹œê°„ì´ í—ˆìš© ì¢…ë£Œ ì‹œê°„ë³´ë‹¤ ëŠ¦ìœ¼ë©´, ë‹¤ìŒ ë‚  ì‹œì‘ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
        if now.hour >= end_hour:
            target_time += datetime.timedelta(days=1)
        
        wait_seconds = (target_time - now).total_seconds()
        logging.info(f"ì‘ì—… í—ˆìš© ì‹œê°„ì´ ì•„ë‹™ë‹ˆë‹¤. {int(wait_seconds // 3600)}ì‹œê°„ {int((wait_seconds % 3600) // 60)}ë¶„ í›„ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        await asyncio.sleep(wait_seconds)

class LawGovKrScraper:
    """
    êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° íŒë¡€ ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤.

    ì´ í´ë˜ìŠ¤ëŠ” íŒë¡€ ëª©ë¡ ì¡°íšŒ, ê°œë³„ íŒë¡€ì˜ PDF ë‹¤ìš´ë¡œë“œ, í…ìŠ¤íŠ¸ ì¶”ì¶œ,
    íŒŒì¼ ì €ì¥ ë“± ìŠ¤í¬ë˜í•‘ì˜ ëª¨ë“  ê³¼ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

    Args:
        oc_id (str): ë²•ì œì²˜ Open API ì¸ì¦í‚¤.
        request_delay (float): ê° HTTP ìš”ì²­ ì‚¬ì´ì˜ ìµœì†Œ ì§€ì—° ì‹œê°„ (ì´ˆ).
        max_retries (int): ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜.
    """
    def __init__(self, oc_id: str, request_delay: float = 0.2, max_retries: int = 3):
        if not oc_id:
            raise ValueError("OC ID (ì¸ì¦í‚¤)ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. API í‚¤ë¥¼ ì¸ìë¡œ ì „ë‹¬í•´ì£¼ì„¸ìš”.")
        self.oc_id = oc_id
        self.request_delay = request_delay
        self.max_retries = max_retries
        # ë°ì´í„° ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)
        os.makedirs(RECOLLECT_DIR, exist_ok=True)

    async def _make_request(self, session: aiohttp.ClientSession, method: str,
                            url: str, params=None, data=None,
                            headers=None, allow_redirects=True):
        """
        ë¹„ë™ê¸° HTTP ìš”ì²­ì„ ë³´ë‚´ëŠ” ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ.
        RateLimiter, ì¬ì‹œë„ ë¡œì§, ì—ëŸ¬ ë¡œê¹…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

        Returns:
            tuple[bytes | None, aiohttp.ClientResponse.url | None]: (ì‘ë‹µ ë³¸ë¬¸, ìµœì¢… URL)
        """
        await asyncio.sleep(self.request_delay)
        last_exception = None
        request_headers = headers or session.headers

        for attempt in range(self.max_retries):
            try:
                # 2ì´ˆì— 3ë²ˆìœ¼ë¡œ ìš”ì²­ ì†ë„ ì œí•œ
                async with RateLimiter(max_calls=3, per_seconds=2):
                    async with session.request(
                        method, url, params=params, data=data,
                        headers=request_headers, allow_redirects=allow_redirects,
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        response.raise_for_status() # 200ë²ˆëŒ€ ì‘ë‹µì´ ì•„ë‹ˆë©´ ì˜ˆì™¸ ë°œìƒ
                        return await response.read(), response.url
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                last_exception = e
                logging.warning(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries}): {url}, ì˜¤ë¥˜: {e}")
                await asyncio.sleep(2 ** attempt) # ì¬ì‹œë„ ê°„ê²© ì¦ê°€ (Exponential backoff)
        
        logging.error(f"ìµœëŒ€ ì¬ì‹œë„({self.max_retries}íšŒ) í›„ì—ë„ ìš”ì²­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. URL: {url}, ìµœì¢… ì˜¤ë¥˜: {last_exception}")
        return None, None

    async def _parse_list_response(self, content: bytes):
        """
        íŒë¡€ ëª©ë¡ APIì˜ XML ì‘ë‹µì„ íŒŒì‹±í•©ë‹ˆë‹¤.

        Args:
            content (bytes): XML í˜•ì‹ì˜ ì‘ë‹µ ë³¸ë¬¸.

        Returns:
            tuple[list[dict], int]: (íŒë¡€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸, ì „ì²´ íŒë¡€ ìˆ˜)
        """
        if not content:
            return [], 0
        try:
            root = ElementTree.fromstring(content)
            # 'prec' íƒœê·¸ê°€ ê° íŒë¡€ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŒ
            data = [{child.tag: child.text for child in item} for item in root.findall('prec')]
            total_count_element = root.find('totalCnt')
            total_count = int(total_count_element.text) if total_count_element is not None else 0
            return data, total_count
        except ElementTree.ParseError as e:
            logging.error(f"XML íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            # API ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸
            try:
                root = ElementTree.fromstring(content)
                error_msg = root.find('.//resultMessage') or root.find('.//MESSAGE')
                if error_msg is not None:
                    logging.error(f"API ì—ëŸ¬ ë©”ì‹œì§€: {error_msg.text}")
            except Exception:
                pass
            return [], 0

    async def fetch_case_list(self, session: aiohttp.ClientSession,
                              query=None, date=None, date_range=None,
                              display=100, max_pages=None):
        """
        íŒë¡€ ëª©ë¡ì„ ì¡°íšŒí•˜ê³  ëª¨ë“  í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

        Args:
            session: aiohttp í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜.
            query (str, optional): ê²€ìƒ‰ì–´.
            date (str, optional): ì„ ê³ ì¼ì (ì˜ˆ: '2023.01.01').
            date_range (str, optional): íŒì‹œì‚¬í•­ ê²Œì¬ì¼ì ë²”ìœ„ (ì˜ˆ: '20230101~20230131').
            display (int): í•œ í˜ì´ì§€ì— í‘œì‹œí•  í•­ëª© ìˆ˜ (ìµœëŒ€ 100).
            max_pages (int, optional): ìˆ˜ì§‘í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜. Noneì´ë©´ ì „ì²´ í˜ì´ì§€ë¥¼ ìˆ˜ì§‘.

        Returns:
            pd.DataFrame: ìˆ˜ì§‘ëœ íŒë¡€ ëª©ë¡ ë°ì´í„°í”„ë ˆì„.
        """
        if not any([query, date, date_range]):
            logging.warning("ê²€ìƒ‰ ì¡°ê±´(query, date, date_range) ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.")
            return pd.DataFrame()

        params = {
            'OC': self.oc_id,
            'target': 'prec',
            'type': 'xml',
            'display': display,
            'page': 1
        }
        if query: params['query'] = query
        if date: params['precJoYd'] = date.replace('.', '')
        if date_range: params['prncYd'] = date_range

        logging.info(f"íŒë¡€ ëª©ë¡ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤. ê²€ìƒ‰ ì¡°ê±´: { {k:v for k,v in params.items() if k not in ['OC', 'target']} }")
        
        # ì²« í˜ì´ì§€ ìš”ì²­ìœ¼ë¡œ ì „ì²´ ê°œìˆ˜ í™•ì¸
        content, _ = await self._make_request(session, 'GET', BASE_URL_SEARCH, params=params)
        
        if content:
            try:
                root = ElementTree.fromstring(content)
                msg = root.find('message') or root.find('msg')
                if msg is not None and msg.text:
                    logging.info(f"API ì‘ë‹µ ë©”ì‹œì§€: {msg.text}")
            except Exception:
                pass

        initial_data, total_items = await self._parse_list_response(content)
        if total_items == 0:
            logging.info("ê²€ìƒ‰ëœ íŒë¡€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        total_pages = (total_items + display - 1) // display
        pages_to_fetch = min(max_pages, total_pages) if max_pages else total_pages
        logging.info(f"ì´ {total_items}ê±´ì˜ íŒë¡€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. {pages_to_fetch}í˜ì´ì§€ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

        all_data = initial_data
        
        # 2í˜ì´ì§€ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ìš”ì²­ (APIê°€ ë™ì‹œ ìš”ì²­ì„ í—ˆìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
        if pages_to_fetch > 1:
            pbar = tqdm(range(2, pages_to_fetch + 1), desc="â¡ï¸  íŒë¡€ ëª©ë¡ í˜ì´ì§•")
            for page in pbar:
                params['page'] = page
                pbar.set_postfix_str(f"í˜ì´ì§€ {page}/{pages_to_fetch}")
                page_content, _ = await self._make_request(session, 'GET', BASE_URL_SEARCH, params=params)
                data, _ = await self._parse_list_response(page_content)
                all_data.extend(data)
                await asyncio.sleep(0.5) # í˜ì´ì§€ ê°„ ì˜ˆì˜ ìˆëŠ” ë”œë ˆì´

        df = pd.DataFrame(all_data)
        logging.info(f"íŒë¡€ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(df)}ê±´")
        return df

    def _sanitize_filename(self, name: str) -> str:
        """íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” íŠ¹ìˆ˜ë¬¸ìë¥¼ '_'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤."""
        return re.sub(r'[\\/*?:"<>|]', '_', str(name)).strip()

    async def _extract_text_from_pdf(self, pdf_bytes: bytes) -> str | None:
        """
        PDF íŒŒì¼ì˜ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
        """
        if not pdf_bytes:
            return None
        
        for attempt in range(self.max_retries):
            try:
                # pdfplumberë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                with io.BytesIO(pdf_bytes) as pdf_file, pdfplumber.open(pdf_file) as pdf:
                    full_text = "\n".join(page.extract_text() or "" for page in pdf.pages)
                    return full_text
            except Exception as e:
                logging.warning(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries}): {e}")
                await asyncio.sleep(1)
        
        logging.error("ìµœëŒ€ ì¬ì‹œë„ í›„ì—ë„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œì— ìµœì¢… ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None

    async def _download_pdf_from_nts(self, session, final_url, case_row):
        """êµ­ì„¸ì²­(NTS) ì›¹ì‚¬ì´íŠ¸ë¡œë¶€í„° PDFë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        parsed_url = urlparse(str(final_url))
        query_params = parse_qs(parsed_url.query)
        ntst_dcm_id = query_params.get('ntstDcmId', [None])[0]
        
        if not ntst_dcm_id:
            logging.warning(f"êµ­ì„¸ì²­ PDF ë‹¤ìš´ë¡œë“œì— í•„ìš”í•œ 'ntstDcmId'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URL: {final_url}")
            return None
            
        post_data = {
            'data': json.dumps({'dcmDVO': {'ntstDcmId': ntst_dcm_id}}),
            'actionId': 'ASIQTB002PR02',
            'fileType': 'pdf',
            'fileName': case_row.get('ì‚¬ê±´ëª…', 'download')
        }
        headers = {
            'Content-Type': 'application/x-www-form-urlencoded',
            'Referer': str(final_url),
            'Origin': f"{parsed_url.scheme}://{parsed_url.netloc}"
        }
        
        pdf_bytes, _ = await self._make_request(
            session, 'POST', NTS_PDF_DOWNLOAD_URL, data=post_data, headers=headers
        )
        return pdf_bytes

    async def _download_pdf_from_lawgo(self, session, final_url, case_row):
        """ë²•ì œì²˜(law.go.kr) ì›¹ì‚¬ì´íŠ¸ë¡œë¶€í„° PDFë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        post_data = {
            'precSeq': case_row['íŒë¡€ì¼ë ¨ë²ˆí˜¸'],
            'fileType': 'pdf',
            'preview': 'N',
            'conJo': '1,2,3,4,5' # PDFì— í¬í•¨í•  í•­ëª© (íŒì‹œì‚¬í•­, íŒê²°ìš”ì§€ ë“±)
        }
        headers = {
            'Content-Type': 'application/x-www-form-urlencoded',
            'Referer': str(final_url),
            'Origin': 'https://www.law.go.kr'
        }
        
        pdf_bytes, _ = await self._make_request(
            session, 'POST', LAWGO_PDF_DOWNLOAD_URL, data=post_data, headers=headers
        )
        return pdf_bytes

    async def _fetch_and_save_case(self, session, case_row, output_dir):
        """
        ê°œë³„ íŒë¡€ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•˜ì—¬ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
        
        ì§„í–‰ ê³¼ì •:
        1. íŒë¡€ì¼ë ¨ë²ˆí˜¸ë¡œ ìƒì„¸ ì •ë³´ í˜ì´ì§€(ê»ë°ê¸°) ìš”ì²­
        2. í˜ì´ì§€ HTMLì—ì„œ ì‹¤ì œ ë‚´ìš©ì´ ë‹´ê¸´ iframe URL ì¶”ì¶œ
        3. iframe URLë¡œ ì ‘ê·¼í•˜ì—¬ ìµœì¢… ì½˜í…ì¸  í˜ì´ì§€ URL íšë“ (ë¦¬ë””ë ‰ì…˜ ì²˜ë¦¬)
        4. ìµœì¢… URLì´ êµ­ì„¸ì²­/ë²•ì œì²˜ì¸ì§€ì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ PDF ë‹¤ìš´ë¡œë“œ
        5. ë‹¤ìš´ë¡œë“œí•œ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        6. PDFì™€ TXT íŒŒì¼ì„ ì§€ì •ëœ ë””ë ‰í† ë¦¬ì— ì €ì¥

        Returns:
            dict: ì‘ì—… ê²°ê³¼ (ìƒíƒœ, íŒë¡€ ì •ë³´, ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë“±)
        """
        case_id = case_row['íŒë¡€ì¼ë ¨ë²ˆí˜¸']
        
        # 1. ìƒì„¸ ì •ë³´ í˜ì´ì§€(ê»ë°ê¸°) ìš”ì²­
        params = {'OC': self.oc_id, 'target': 'prec', 'type': 'HTML', 'ID': case_id}
        shell_content, shell_url = await self._make_request(
            session, 'GET', BASE_URL_SERVICE, params=params
        )
        if not shell_content:
            return {'status': 'DOWNLOAD_FAIL', 'case_info': case_row, 'reason': 'Shell page fetch failed'}

        # 2. iframe URL ì¶”ì¶œ
        soup = BeautifulSoup(shell_content, 'html.parser')
        url_input = soup.find('input', {'type': 'hidden', 'id': 'url'})
        path = url_input['value'] if url_input else None
        if not path:
            iframe = soup.find('iframe')
            path = iframe['src'] if iframe else None
        
        if not path:
            return {'status': 'DOWNLOAD_FAIL', 'case_info': case_row, 'reason': 'Could not find content URL/iframe'}

        # 3. ì‹¤ì œ ì½˜í…ì¸  í˜ì´ì§€ URL íšë“
        page_url = urljoin(str(shell_url), path)
        _, final_url = await self._make_request(
            session, 'GET', page_url, headers={'Referer': str(shell_url)}
        )
        if not final_url:
            return {'status': 'DOWNLOAD_FAIL', 'case_info': case_row, 'reason': 'Content page fetch failed'}

        # 4. PDF ë‹¤ìš´ë¡œë“œ
        if 'taxlaw.nts.go.kr' in str(final_url):
            pdf_bytes = await self._download_pdf_from_nts(session, final_url, case_row)
        else:
            pdf_bytes = await self._download_pdf_from_lawgo(session, final_url, case_row)
        
        if not pdf_bytes:
            return {'status': 'DOWNLOAD_FAIL', 'case_info': case_row, 'reason': 'PDF download failed'}

        # 5. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = await self._extract_text_from_pdf(pdf_bytes)
        if not text:
            return {'status': 'PARSE_FAIL', 'case_info': case_row}

        # 6. íŒŒì¼ ì €ì¥
        filename = f"{case_id}_{self._sanitize_filename(case_row.get('ì‚¬ê±´ë²ˆí˜¸', ''))}"
        pdf_path = os.path.join(output_dir, f"{filename}.pdf")
        txt_path = os.path.join(output_dir, f"{filename}.txt")
        
        try:
            async with aiofiles.open(pdf_path, 'wb') as f:
                await f.write(pdf_bytes)
            async with aiofiles.open(txt_path, 'w', encoding='utf-8') as f:
                await f.write(text)
        except Exception as e:
            logging.error(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ID: {case_id}): {e}")
            # ì‹¤íŒ¨ ì‹œ ìƒì„±ëœ íŒŒì¼ ì‚­ì œ ì‹œë„
            if await aio_os.path.exists(pdf_path): await aio_os.remove(pdf_path)
            if await aio_os.path.exists(txt_path): await aio_os.remove(txt_path)
            return {'status': 'SAVE_FAIL', 'case_info': case_row}

        return {'status': 'SUCCESS', 'case_info': case_row, 'text': text}

    async def _process_cases_batch(self, session, df, output_dir):
        """
        íŒë¡€ ëª©ë¡(DataFrame)ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•©ë‹ˆë‹¤.
        """
        if df.empty:
            return pd.DataFrame(), {}
            
        logging.info(f"íŒë¡€ ë³¸ë¬¸ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤: {len(df)}ê±´ -> {output_dir}")
        
        tasks = [self._fetch_and_save_case(session, row, output_dir) for _, row in df.iterrows()]
        
        results = []
        with tqdm(total=len(tasks), desc="âœï¸  íŒë¡€ ë³¸ë¬¸ ìˆ˜ì§‘") as pbar:
            for coro in asyncio.as_completed(tasks):
                result = await coro
                results.append(result)
                pbar.update(1)

        status_map = {
            'SUCCESS': [], 'DOWNLOAD_FAIL': [],
            'PARSE_FAIL': [], 'SAVE_FAIL': [], 'SKIPPED_EXISTS': []
        }
        texts = []
        # ê²°ê³¼ë¥¼ case_id ê¸°ì¤€ìœ¼ë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬
        result_map = {r['case_info']['íŒë¡€ì¼ë ¨ë²ˆí˜¸']: r for r in results}

        for _, row in df.iterrows():
            case_id = row['íŒë¡€ì¼ë ¨ë²ˆí˜¸']
            result = result_map.get(case_id)
            if result:
                status_map.setdefault(result['status'], []).append(result['case_info'])
                texts.append(result.get('text'))
            else:
                # ì´ ê²½ìš°ëŠ” ê±°ì˜ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨
                texts.append(None)

        out_df = df.copy()
        out_df['íŒë¡€ë³¸ë¬¸'] = texts
        return out_df, status_map

    async def _write_period_summary(self, output_dir, suffix, status_map):
        """
        ê¸°ê°„ë³„ ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ìš”ì•½í•œ ë¦¬í¬íŠ¸ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        success_list = status_map.get('SUCCESS', [])
        skipped_list = status_map.get('SKIPPED_EXISTS', [])
        total_success = len(success_list) + len(skipped_list)
        total_processed = sum(len(v) for v in status_map.values())
        fail_count = total_processed - total_success

        report_path = os.path.join(output_dir, f"crawling_result_{suffix}.txt")
        async with aiofiles.open(report_path, 'w', encoding='utf-8') as f:
            await f.write(f"--- ìˆ˜ì§‘ ìš”ì•½ ({suffix}) ---\n")
            await f.write(f"ì´ ëŒ€ìƒ: {total_processed}ê±´\n")
            await f.write(f"ìµœì¢… ì„±ê³µ: {total_success}ê±´ (ì‹ ê·œ: {len(success_list)}ê±´, ì¤‘ë³µ/ê±´ë„ˆëœ€: {len(skipped_list)}ê±´)\n")
            await f.write(f"ìˆ˜ì§‘ ì‹¤íŒ¨: {fail_count}ê±´\n\n")

            if success_list:
                await f.write(f"[ì‹ ê·œ ìˆ˜ì§‘ ì„±ê³µ: {len(success_list)}ê±´]\n")
                for item in success_list:
                    await f.write(f"- ID: {item['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}, ì‚¬ê±´ë²ˆí˜¸: {item['ì‚¬ê±´ë²ˆí˜¸']}\n")
                await f.write("\n")
            
            # ì‹¤íŒ¨ ìƒì„¸ ë‚´ì—­ ì‘ì„±
            for key, label in [('DOWNLOAD_FAIL', 'ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨'),
                               ('PARSE_FAIL', 'í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨'),
                               ('SAVE_FAIL', 'íŒŒì¼ ì €ì¥ ì‹¤íŒ¨')]:
                arr = status_map.get(key, [])
                if arr:
                    await f.write(f"[{label}: {len(arr)}ê±´]\n")
                    for item in arr:
                        await f.write(f"- ID: {item['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}, ì‚¬ê±´ë²ˆí˜¸: {item['ì‚¬ê±´ë²ˆí˜¸']}\n")
                    await f.write("\n")
        
        logging.info(f"ê¸°ê°„ë³„ ë¦¬í¬íŠ¸ ì‘ì„± ì™„ë£Œ: {report_path}")

async def run_scraper(oc_id: str, query: str = None, date: str = None, 
                      date_range: str = None, test_mode: bool = False):
    """
    íŒë¡€ ìŠ¤í¬ë˜í¼ë¥¼ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.

    Args:
        oc_id (str): ë²•ì œì²˜ Open API ì¸ì¦í‚¤ (í•„ìˆ˜).
        query (str, optional): ê²€ìƒ‰ì–´. Defaults to None.
        date (str, optional): ì„ ê³ ì¼ì (ì˜ˆ: '2023.12.25'). Defaults to None.
        date_range (str, optional): íŒì‹œì‚¬í•­ ê²Œì¬ì¼ì ë²”ìœ„ (ì˜ˆ: '20230101~20231231'). Defaults to None.
        test_mode (bool, optional): í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€. Defaults to False.
            - Trueì¼ ê²½ìš°, ì‹œê°„ ì œì•½ ì—†ì´ 'íŒë¡€' ê²€ìƒ‰ì–´ë¡œ 1í˜ì´ì§€ë§Œ ìˆ˜ì§‘.
    """
    # --- ëª¨ë“œ ì„¤ì •: ì¼ë°˜ ëª¨ë“œ vs í…ŒìŠ¤íŠ¸ ëª¨ë“œ ---
    if test_mode:
        logging.info("--- ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™” ğŸ§ª ---")
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œìš© íŒŒë¼ë¯¸í„° ì„¤ì •
        query_param = "íŒë¡€"
        date_param = None
        date_range_param = None
        max_pages = 1  # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” 1í˜ì´ì§€ë§Œ ìˆ˜ì§‘
        output_suffix = f"test_{datetime.datetime.now():%Y%m%d_%H%M%S}"
    else:
        # ì¼ë°˜ ëª¨ë“œìš© íŒŒë¼ë¯¸í„° ì„¤ì •
        if not any([query, date, date_range]):
            logging.error("ì¼ë°˜ ëª¨ë“œì—ì„œëŠ” query, date, date_range ì¤‘ í•˜ë‚˜ ì´ìƒì˜ ê²€ìƒ‰ ì¡°ê±´ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        query_param = query
        date_param = date
        date_range_param = date_range
        max_pages = None # ì „ì²´ í˜ì´ì§€ ìˆ˜ì§‘
        output_suffix = date.replace('.', '') if date else (date_range or "query_search")

    # --- í¬ë¡¤ëŸ¬ ì‹¤í–‰ ---
    scraper = LawGovKrScraper(oc_id=oc_id)

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ ì‘ì—… í—ˆìš© ì‹œê°„ê¹Œì§€ ëŒ€ê¸°
    if not test_mode:
        await wait_for_window()

    async with aiohttp.ClientSession(headers=DEFAULT_HEADERS) as session:
        # 1. íŒë¡€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        df_list = await scraper.fetch_case_list(
            session, query=query_param, date=date_param, 
            date_range=date_range_param, max_pages=max_pages
        )

        if df_list.empty:
            logging.info("ìˆ˜ì§‘í•  íŒë¡€ ëª©ë¡ì´ ì—†ì–´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        # 2. íŒŒì¼ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ í›„, ëˆ„ë½ëœ íŒë¡€ë§Œ ìˆ˜ì§‘
        output_dir = os.path.join(BASE_OUTPUT_DIR, output_suffix)
        os.makedirs(output_dir, exist_ok=True)
        
        cases_to_process = []
        skipped_case_infos = []
        logging.info(f"ê¸°ì¡´ íŒŒì¼ í™•ì¸ ë° ìˆ˜ì§‘ ëŒ€ìƒ í•„í„°ë§ ì¤‘... (ì´ {len(df_list)}ê±´)")
        for _, row in df_list.iterrows():
            filename = f"{row['íŒë¡€ì¼ë ¨ë²ˆí˜¸']}_{scraper._sanitize_filename(row.get('ì‚¬ê±´ë²ˆí˜¸', ''))}"
            txt_path = os.path.join(output_dir, f"{filename}.txt")
            
            if os.path.exists(txt_path):
                skipped_case_infos.append(row.to_dict())
            else:
                cases_to_process.append(row.to_dict())

        logging.info(f"ì¤‘ë³µ íŒŒì¼ ì œì™¸, ì‹ ê·œ ìˆ˜ì§‘ ëŒ€ìƒ: {len(cases_to_process)}ê±´ (ê±´ë„ˆë›°ê¸°: {len(skipped_case_infos)}ê±´)")

        status_map = {}
        if cases_to_process:
            df_to_process = pd.DataFrame(cases_to_process)
            _, status_map = await scraper._process_cases_batch(session, df_to_process, output_dir)
        else:
            logging.info("ì‹ ê·œë¡œ ìˆ˜ì§‘í•  íŒë¡€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ê±´ë„ˆë›´ í•­ëª©ì„ status_mapì— ì¶”ê°€í•˜ì—¬ ë¦¬í¬íŠ¸ì— ë°˜ì˜
        status_map['SKIPPED_EXISTS'] = skipped_case_infos
        
        # 3. ìˆ˜ì§‘ ê²°ê³¼ ë¦¬í¬íŠ¸ ì‘ì„±
        await scraper._write_period_summary(output_dir, output_suffix, status_map)
        
        # 4. ìµœì¢… ê²°ê³¼ ë¡œê¹… (ì¬ìˆ˜ì§‘ ë¡œì§ì€ ì œê±°ë¨)
        all_failed_items = (status_map.get('DOWNLOAD_FAIL', []) + 
                            status_map.get('PARSE_FAIL', []) + 
                            status_map.get('SAVE_FAIL', []))

        if not all_failed_items and cases_to_process:
            logging.info("ëª¨ë“  ì‹ ê·œ í•­ëª©ì´ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        elif not all_failed_items and not cases_to_process:
            logging.info("ì²˜ë¦¬í•  ì‹ ê·œ í•­ëª©ì´ ì—†ì—ˆê³ , ì‹¤íŒ¨ë„ ì—†ì—ˆìŠµë‹ˆë‹¤.")
        else:
            logging.warning(f"ì´ {len(all_failed_items)}ê±´ì˜ í•­ëª© ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒì„¸ ë‚´ìš©ì€ ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    logging.info("âœ¨ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. âœ¨")


if __name__ == "__main__":
    # --- ì‹¤í–‰ ì˜ˆì‹œ ---
    # ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „, ì•„ë˜ YOUR_OC_ID ë³€ìˆ˜ì— ì‹¤ì œ ë²•ì œì²˜ Open API ì¸ì¦í‚¤ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
    
    YOUR_OC_ID = "leegy76" 

    if YOUR_OC_ID == "YOUR_API_KEY_HERE":
        logging.error("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— 'YOUR_OC_ID' ë³€ìˆ˜ì— ì‹¤ì œ API ì¸ì¦í‚¤ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
    else:
        # --- ì‹¤í–‰ ì˜µì…˜ (ì•„ë˜ ì¤‘ ì‹¤í–‰í•˜ë ¤ëŠ” ì˜µì…˜ì˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”) ---
        try:
            # ì˜µì…˜ 1: í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰ (ì‹œê°„ ì œì•½ ì—†ì´ 'íŒë¡€' ê²€ìƒ‰ì–´ë¡œ 1í˜ì´ì§€ë§Œ ìˆ˜ì§‘)
            asyncio.run(run_scraper(oc_id=YOUR_OC_ID, test_mode=True))

            # ì˜µì…˜ 2: íŠ¹ì • ë‚ ì§œë¡œ ê²€ìƒ‰í•˜ì—¬ ì‹¤í–‰
            # asyncio.run(run_scraper(oc_id=YOUR_OC_ID, date="2023.01.05"))

            # ì˜µì…˜ 3: íŠ¹ì • ê¸°ê°„ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ì‹¤í–‰
            # asyncio.run(run_scraper(oc_id=YOUR_OC_ID, date_range="20230101~20230131"))
            
            # ì˜µì…˜ 4: íŠ¹ì • ê²€ìƒ‰ì–´ë¡œ ì‹¤í–‰
            # asyncio.run(run_scraper(oc_id=YOUR_OC_ID, query="ë¯¼ë²•"))

        except KeyboardInterrupt:
            logging.info("ì‚¬ìš©ìì— ì˜í•´ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logging.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
