import pandas as pd
import os
import time
import re
from xml.etree import ElementTree # ë²•ë ¹ ëª©ë¡ ì¡°íšŒë¥¼ ìœ„í•´ ë‹¤ì‹œ ì‚¬ìš©
import json
from tqdm.asyncio import tqdm
from urllib.parse import urljoin, parse_qs, urlparse
import io
import asyncio
import aiohttp
import aiofiles
import aiofiles.os as aio_os

# --- ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸ ---
class IPBlockedError(Exception):
    """IP ì ‘ê·¼ ì œí•œ ì‹œ ë°œìƒí•˜ëŠ” ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸"""
    pass

# --- ìƒìˆ˜ ì •ì˜ ---
DEFAULT_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Connection': 'keep-alive',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'
}
BASE_URL_SEARCH = "http://www.law.go.kr/DRF/lawSearch.do"
BASE_URL_SERVICE = "http://www.law.go.kr/DRF/lawService.do"
BASE_OUTPUT_DIR = os.path.join("data", "raw_laws")


class LawScraper:
    """
    ëŒ€í•œë¯¼êµ­ ë²•ì œì²˜ êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°ì˜ ë²•ë ¹ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë¹„ë™ê¸° ìŠ¤í¬ë ˆì´í¼.
    (IP ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ë™ì‹œì„± ì œì–´ ë° ì¬ì‹œë„ ë¡œì§ ê°•í™”)
    """

    def __init__(self, oc_id, request_delay=1.0, max_retries=3, max_concurrency=5):
        if not oc_id:
            raise ValueError("OC ID (ì¸ì¦í‚¤)ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
        self.oc_id = oc_id
        self.request_delay = request_delay # ìš”ì²­ ê°„ ë”œë ˆì´ ì¦ê°€
        self.max_retries = max_retries
        self.semaphore = asyncio.Semaphore(max_concurrency) # ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ

        os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)

    async def _make_request(self, session, method, url, params=None, data=None, headers=None, allow_redirects=True):
        """ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¹„ë™ê¸° ìš”ì²­ ì²˜ë¦¬ ë©”ì„œë“œ (ë™ì‹œì„± ì œì–´ ë° IP ì°¨ë‹¨ ê°ì§€ ì¶”ê°€)"""
        async with self.semaphore: # í•œ ë²ˆì— ì •í•´ì§„ ìˆ˜ì˜ ìš”ì²­ë§Œ ì‹¤í–‰
            await asyncio.sleep(self.request_delay)
            last_exception = None
            req_headers = headers or session.headers
            
            for attempt in range(self.max_retries):
                try:
                    async with session.request(method, url, params=params, data=data, headers=req_headers,
                                               allow_redirects=allow_redirects, timeout=30) as response:
                        response.raise_for_status()
                        content = await response.read()

                        # IP ì°¨ë‹¨ ë©”ì‹œì§€ í™•ì¸
                        if b'IP' in content and b'alert' in content:
                            decoded_content = content.decode('utf-8', errors='ignore')
                            if 'ì ‘ê·¼ì œí•œëœ IP ì…ë‹ˆë‹¤' in decoded_content:
                                raise IPBlockedError("IPê°€ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")

                        return content, response.url
                except (aiohttp.ClientError, asyncio.TimeoutError, IPBlockedError) as e:
                    last_exception = e
                    tqdm.write(f"âš ï¸ ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries}): {e}")
                    if attempt < self.max_retries - 1:
                        # ì§€ìˆ˜ ë°±ì˜¤í”„: 2, 4, 8ì´ˆ... ìˆœìœ¼ë¡œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                        wait_time = 2 ** (attempt + 1) 
                        tqdm.write(f"   â¡ï¸ {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                        await asyncio.sleep(wait_time)
                    else:
                        tqdm.write(f"âŒ ìµœëŒ€ ì¬ì‹œë„({self.max_retries}íšŒ) í›„ ìš”ì²­ ìµœì¢… ì‹¤íŒ¨. URL: {url}, ì˜¤ë¥˜: {last_exception}")
            return None, None

    async def _parse_law_list_xml_response(self, response_content):
        """ë²•ë ¹ ëª©ë¡ APIì˜ XML ì‘ë‹µì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        if not response_content: return [], 0
        try:
            root = ElementTree.fromstring(response_content)
            data = [{child.tag: child.text for child in item} for item in root.findall('law')]
            total_count_element = root.find('totalCnt')
            total_count = int(total_count_element.text) if total_count_element is not None else 0
            return data, total_count
        except Exception as e:
            tqdm.write(f" âŒ XML íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return [], 0

    async def fetch_law_list(self, session, efyd_range=None, display=100, max_pages=None):
        """ì‹œí–‰ì¼ì ë²”ìœ„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²•ë ¹ ëª©ë¡ì„ ë¹„ë™ê¸°ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤. (XML ë°©ì‹)"""
        if not efyd_range:
            print("âš ï¸ ì‹œí–‰ì¼ì ë²”ìœ„(efyd_range)ëŠ” ë°˜ë“œì‹œ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
            return pd.DataFrame()

        print(f"â¡ï¸ ì‹œí–‰ì¼ì ë²”ìœ„ '{efyd_range}'ì— ëŒ€í•œ ë²•ë ¹ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘... (XML)")

        params = {
            'OC': self.oc_id, 
            'target': 'law', 
            'type': 'xml',
            'display': display, 
            'page': 1,
            'sort': 'efdesc',
            'efYd': efyd_range 
        }

        content, _ = await self._make_request(session, 'GET', BASE_URL_SEARCH, params=params)
        if not content: return pd.DataFrame()

        initial_data, total_count = await self._parse_law_list_xml_response(content)
        if total_count == 0:
            print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        all_data = initial_data
        total_pages = (total_count + display - 1) // display
        pages_to_fetch = min(max_pages, total_pages) if max_pages is not None else total_pages
        print(f"ğŸ“Š ì´ {total_count}ê°œì˜ ë²•ë ¹ ë°œê²¬. {pages_to_fetch}í˜ì´ì§€ì— ê±¸ì³ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

        if pages_to_fetch > 1:
            tasks = [self._make_request(session, 'GET', BASE_URL_SEARCH, params={**params, 'page': i}) for i in range(2, pages_to_fetch + 1)]
            for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="ğŸ“– ë²•ë ¹ ëª©ë¡ ìˆ˜ì§‘ ì¤‘"):
                page_content, _ = await f
                if page_content:
                    data, _ = await self._parse_law_list_xml_response(page_content)
                    if data: all_data.extend(data)
        
        df = pd.DataFrame(all_data)
        print(f"\nğŸ‰ ë²•ë ¹ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df)}ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return df

    def _sanitize_filename(self, filename):
        return re.sub(r'[\\/*?:"<>|]', "", str(filename)).strip() if filename else ""

    @staticmethod
    def _parse_law_article_parts(data, indent="  "):
        """í•­/í˜¸ ë“± ì¡°ë¬¸ì˜ í•˜ìœ„ êµ¬ì¡°ë¥¼ ì¬ê·€ì ìœ¼ë¡œ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        parts = []
        if 'í•­' in data:
            í•­_list = data['í•­'] if isinstance(data['í•­'], list) else [data['í•­']]
            for í•­ in í•­_list:
                if 'í•­ë‚´ìš©' in í•­:
                    í•­_ë²ˆí˜¸ = í•­.get('í•­ë²ˆí˜¸', '')
                    í•­_ë‚´ìš© = f"{indent}{í•­_ë²ˆí˜¸} {í•­['í•­ë‚´ìš©'].strip()}" if í•­_ë²ˆí˜¸ else f"{indent}{í•­['í•­ë‚´ìš©'].strip()}"
                    parts.append(í•­_ë‚´ìš©)
                parts.extend(LawScraper._parse_law_article_parts(í•­, indent + "  "))
        if 'í˜¸' in data:
            í˜¸_list = data['í˜¸'] if isinstance(data['í˜¸'], list) else [data['í˜¸']]
            for í˜¸ in í˜¸_list:
                if 'í˜¸ë‚´ìš©' in í˜¸:
                    í˜¸_ë²ˆí˜¸ = í˜¸.get('í˜¸ë²ˆí˜¸', '')
                    í˜¸_ë‚´ìš© = f"{indent}{í˜¸_ë²ˆí˜¸} {í˜¸['í˜¸ë‚´ìš©'].strip()}" if í˜¸_ë²ˆí˜¸ else f"{indent}{í˜¸['í˜¸ë‚´ìš©'].strip()}"
                    parts.append(í˜¸_ë‚´ìš©)
        return parts

    async def _format_json_to_text(self, json_data):
        """ë²•ë ¹ ë³¸ë¬¸ JSONì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ í¬ë§·íŒ…í•©ë‹ˆë‹¤. (êµ¬ì¡° ë¶„ì„ ë° ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”)"""
        try:
            law_data = json_data.get("ë²•ë ¹", json_data)
            text_parts = []

            text_parts.append("="*20 + " ê¸°ë³¸ ì •ë³´ " + "="*20)
            basic_info = law_data.get('ê¸°ë³¸ì •ë³´', {})
            if basic_info:
                order = ['ë²•ë ¹ëª…_í•œê¸€', 'ë²•ì¢…êµ¬ë¶„', 'ì†Œê´€ë¶€ì²˜', 'ê³µí¬ë²ˆí˜¸', 'ê³µí¬ì¼ì', 'ì‹œí–‰ì¼ì', 'ì œê°œì •êµ¬ë¶„']
                for key in order:
                    if key in basic_info:
                        value = basic_info[key]
                        if isinstance(value, dict):
                            content = value.get('content', '')
                            if content: text_parts.append(f"[{key}] {content.strip()}")
                        elif value and isinstance(value, str):
                            text_parts.append(f"[{key}] {value.strip()}")

            reason_info = law_data.get('ì œê°œì •ì´ìœ ', {})
            if reason_info and 'ì œê°œì •ì´ìœ ë‚´ìš©' in reason_info:
                text_parts.append("\n" + "="*20 + " ì œê°œì • ì´ìœ  " + "="*20)
                reason_content = [line.strip() for sublist in reason_info.get('ì œê°œì •ì´ìœ ë‚´ìš©', []) for line in sublist]
                text_parts.append("\n".join(reason_content))

            revision_info = law_data.get('ê°œì •ë¬¸', {})
            if revision_info and 'ê°œì •ë¬¸ë‚´ìš©' in revision_info:
                text_parts.append("\n" + "="*20 + " ê°œì •ë¬¸ " + "="*20)
                revision_content = [line.strip() for sublist in revision_info.get('ê°œì •ë¬¸ë‚´ìš©', []) for line in sublist]
                text_parts.append("\n".join(revision_content))

            text_parts.append("\n" + "="*20 + " ì¡°ë¬¸ ì •ë³´ " + "="*20)
            article_container = law_data.get('ì¡°ë¬¸')
            articles = []
            if isinstance(article_container, dict):
                articles = article_container.get('ì¡°ë¬¸ë‹¨ìœ„', [])
            elif isinstance(article_container, list):
                articles = article_container

            if not articles:
                text_parts.append("ì¶”ì¶œëœ ì¡°ë¬¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                for article in articles:
                    article_parts = []
                    clean_content = lambda s: re.sub(r'^\s*<!\[CDATA\[|\]\]>\s*$', '', s.strip())
                    content = clean_content(article.get('ì¡°ë¬¸ë‚´ìš©', ''))
                    article_parts.append(content)
                    sub_parts = self._parse_law_article_parts(article)
                    if sub_parts:
                        article_parts.extend(sub_parts)
                    text_parts.append("\n".join(article_parts))

            appendix_info = law_data.get('ë¶€ì¹™', {})
            if appendix_info and 'ë¶€ì¹™ë‹¨ìœ„' in appendix_info:
                text_parts.append("\n" + "="*20 + " ë¶€ì¹™ ì •ë³´ " + "="*20)
                appendix_units = appendix_info.get('ë¶€ì¹™ë‹¨ìœ„', [])
                if not isinstance(appendix_units, list):
                    appendix_units = [appendix_units]
                for appendix_unit in appendix_units:
                    if isinstance(appendix_unit, dict):
                        p_num = appendix_unit.get('ë¶€ì¹™ê³µí¬ë²ˆí˜¸', '')
                        p_date = appendix_unit.get('ë¶€ì¹™ê³µí¬ì¼ì', '')
                        text_parts.append(f"\n--- ë¶€ì¹™ (ê³µí¬ë²ˆí˜¸: {p_num}, ê³µí¬ì¼ì: {p_date}) ---")
                        raw_content = appendix_unit.get('ë¶€ì¹™ë‚´ìš©', [])
                        appendix_content_list = []
                        if isinstance(raw_content, list):
                            for item in raw_content:
                                if isinstance(item, list):
                                    appendix_content_list.extend([line.strip() for line in item])
                                elif isinstance(item, str):
                                    appendix_content_list.append(item.strip())
                        elif isinstance(raw_content, str):
                            appendix_content_list.append(raw_content.strip())
                        text_parts.append("\n".join(appendix_content_list))
                    elif isinstance(appendix_unit, str):
                        text_parts.append(f"\n--- ë¶€ì¹™ ---")
                        text_parts.append(appendix_unit.strip())

            return "\n\n".join(text_parts)
        except Exception as e:
            tqdm.write(f" âŒ ë²•ë ¹ ë³¸ë¬¸ JSON í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def _fetch_and_save_law(self, session, law_row, period_output_dir):
        """ë‹¨ì¼ ë²•ë ¹ì˜ ë³¸ë¬¸ì„ ìˆ˜ì§‘í•˜ê³  í…ìŠ¤íŠ¸ íŒŒì¼ê³¼ JSON ì›ë³¸ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. (JSON ë°©ì‹)"""
        law_id = law_row['ë²•ë ¹ID']
        params = {'OC': self.oc_id, 'target': 'law', 'type': 'JSON', 'ID': law_id}
        
        response_content, _ = await self._make_request(session, 'GET', BASE_URL_SERVICE, params=params)
        if not response_content:
            return {'status': 'DOWNLOAD_FAIL', 'law_info': law_row}

        try:
            json_data = json.loads(response_content)
        except json.JSONDecodeError:
            return {'status': 'PARSE_FAIL', 'law_info': law_row}

        text_content = await self._format_json_to_text(json_data)
        if not text_content:
            return {'status': 'PARSE_FAIL', 'law_info': law_row}

        base_filename = f"{law_row['ë²•ë ¹ID']}_{self._sanitize_filename(law_row['ë²•ë ¹ëª…í•œê¸€'])}"
        txt_path = os.path.join(period_output_dir, f"{base_filename}.txt")
        json_path = os.path.join(period_output_dir, f"{base_filename}.json")
        
        try:
            async with aiofiles.open(txt_path, 'w', encoding='utf-8') as f:
                await f.write(text_content)
            
            async with aiofiles.open(json_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(json_data, indent=4, ensure_ascii=False))

        except Exception as e:
            tqdm.write(f" Â [ì‹¤íŒ¨] ID {law_id}: íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")
            return {'status': 'SAVE_FAIL', 'law_info': law_row}
        
        return {'status': 'SUCCESS', 'text': text_content, 'law_info': law_row}

    async def _process_laws_batch(self, session, law_df, period_output_dir):
        """ì£¼ì–´ì§„ ë°ì´í„°í”„ë ˆì„ì˜ ë²•ë ¹ë“¤ì„ ìˆ˜ì§‘í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not isinstance(law_df, pd.DataFrame) or law_df.empty: return pd.DataFrame(), {}

        print(f"â¡ï¸ {len(law_df)}ê°œ ë²•ë ¹ì˜ ë³¸ë¬¸ ìˆ˜ì§‘ ë° ì €ì¥ ì‹œì‘...")
        
        tasks = [self._fetch_and_save_law(session, row, period_output_dir) for _, row in law_df.iterrows()]
        results = await tqdm.gather(*tasks, desc="âœï¸  ë²•ë ¹ ë³¸ë¬¸ ìˆ˜ì§‘/ì €ì¥ ì¤‘")

        status_map = {'SUCCESS': [], 'DOWNLOAD_FAIL': [], 'PARSE_FAIL': [], 'SAVE_FAIL': []}
        all_texts = []
        for res in results:
            status_map.setdefault(res['status'], []).append(res['law_info'])
            all_texts.append(res.get('text'))
        
        df_with_texts = law_df.copy()
        df_with_texts['ë²•ë ¹ë³¸ë¬¸'] = all_texts
        
        return df_with_texts, status_map

    async def _write_period_summary(self, period_output_dir, filename_suffix, status_map):
        """ìˆ˜ì§‘ ê¸°ê°„ì— ëŒ€í•œ ìš”ì•½ ë¦¬í¬íŠ¸ íŒŒì¼ì„ ì‘ì„±í•©ë‹ˆë‹¤."""
        success_list = status_map.get('SUCCESS', [])
        skipped_list = status_map.get('SKIPPED_EXISTS', [])
        download_fail_list = status_map.get('DOWNLOAD_FAIL', [])
        parse_fail_list = status_map.get('PARSE_FAIL', [])
        save_fail_list = status_map.get('SAVE_FAIL', [])
        
        total_items = sum(len(v) for v in status_map.values())
        newly_success_count = len(success_list)
        total_success_count = newly_success_count + len(skipped_list)
        success_rate = (total_success_count / total_items * 100) if total_items > 0 else 0
        
        summary_path = os.path.join(period_output_dir, f"crawling_result_{filename_suffix}.txt")
        async with aiofiles.open(summary_path, 'w', encoding='utf-8') as f:
            await f.write(f"--- ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ({filename_suffix}) ---\n")
            await f.write(f"ì´ ëŒ€ìƒ: {total_items}ê±´\n")
            await f.write(f"ìµœì¢… ì„±ê³µ: {total_success_count}ê±´ (ì‹ ê·œ ìˆ˜ì§‘: {newly_success_count}ê±´, ê±´ë„ˆë›°ê¸°: {len(skipped_list)}ê±´)\n")
            await f.write(f"ìˆ˜ì§‘ ì‹¤íŒ¨: {total_items - total_success_count}ê±´\n")
            await f.write(f"ì„±ê³µë¥ : {success_rate:.2f}%\n")
            await f.write("\n--- ì„¸ë¶€ ë‚´ì—­ ---\n")
            
            if download_fail_list:
                await f.write(f"\n[ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {len(download_fail_list)}ê±´]\n")
                for item in download_fail_list: await f.write(f"- ID: {item['ë²•ë ¹ID']}, ë²•ë ¹ëª…: {item['ë²•ë ¹ëª…í•œê¸€']}\n")
            if parse_fail_list:
                await f.write(f"\n[JSON íŒŒì‹± ì‹¤íŒ¨: {len(parse_fail_list)}ê±´]\n")
                for item in parse_fail_list: await f.write(f"- ID: {item['ë²•ë ¹ID']}, ë²•ë ¹ëª…: {item['ë²•ë ¹ëª…í•œê¸€']}\n")
            if save_fail_list:
                await f.write(f"\n[íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {len(save_fail_list)}ê±´]\n")
                for item in save_fail_list: await f.write(f"- ID: {item['ë²•ë ¹ID']}, ë²•ë ¹ëª…: {item['ë²•ë ¹ëª…í•œê¸€']}\n")
        print(f"ğŸ“„ ê¸°ê°„ë³„ ë¦¬í¬íŠ¸ê°€ '{summary_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def generate_filename_suffix(**kwargs):
    parts = [f"{k}_{v.replace('.', '-').replace('~', '-')}" for k, v in kwargs.items() if v]
    return "_".join(re.sub(r'[\\/*?:"<>|~]', "_", part) for part in parts)

async def main():
    # ğŸš¨ ì¤‘ìš”: ì—¬ê¸°ì— ë°œê¸‰ë°›ì€ ì¸ì¦í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.
    MY_OC_ID = "leegy76" 
    if MY_OC_ID == "YOUR_OC_ID_HERE":
        print("ğŸš¨ ì½”ë“œ ì‹¤í–‰ ì „ì— 'MY_OC_ID' ë³€ìˆ˜ì— ë°œê¸‰ë°›ì€ ì¸ì¦í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    async def _write_overall_summary(stats_list):
        """ì „ì²´ ìˆ˜ì§‘ í˜„í™©ì„ íŒŒì¼ì— ê¸°ë¡í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        print("\n" + "="*60 + "\nğŸ”„ï¸ ì¢…í•© ìˆ˜ì§‘ í˜„í™© ì—…ë°ì´íŠ¸...\n" + "="*60)
        successful_periods = [s for s in stats_list if s['status'] == 'SUCCESS']
        nodata_periods = [s for s in stats_list if s['status'] == 'NO_DATA']
        failed_periods = [s for s in stats_list if s['status'] == 'FATAL_ERROR']
        
        total_periods = len(stats_list)
        success_rate = (len(successful_periods) + len(nodata_periods)) / total_periods * 100 if total_periods > 0 else 0

        summary_text = [
            f"ì´ ì‹œë„ ê¸°ê°„: {total_periods}ê°œ",
            f"ìˆ˜ì§‘ ì„±ê³µ (ë°ì´í„° ìˆìŒ): {len(successful_periods)}ê°œ",
            f"ìˆ˜ì§‘ ì„±ê³µ (ë°ì´í„° ì—†ìŒ): {len(nodata_periods)}ê°œ",
            f"ìˆ˜ì§‘ ì‹¤íŒ¨: {len(failed_periods)}ê°œ",
            f"ì„±ê³µë¥ : {success_rate:.2f}%",
            "\n--- ê¸°ê°„ë³„ ìƒì„¸ í˜„í™© ---"
        ]
        for stat in stats_list:
            period = stat['period']
            status = stat['status']
            if status == 'SUCCESS':
                s_map = stat['stats']
                total = sum(len(v) for v in s_map.values())
                success_count = len(s_map.get('SUCCESS', [])) + len(s_map.get('SKIPPED_EXISTS', []))
                summary_text.append(f"- {period}: ì„±ê³µ (ì²˜ë¦¬ {success_count}/{total}ê±´)")
            else:
                summary_text.append(f"- {period}: {status}")

        report_str = "\n".join(summary_text)
        print(report_str)
        
        report_path = os.path.join(BASE_OUTPUT_DIR, "crawling_result_overall.txt")
        async with aiofiles.open(report_path, 'w', encoding='utf-8') as f:
            await f.write(report_str)
        print(f"\nğŸ’¾ ì¢…í•© ë¦¬í¬íŠ¸ê°€ '{report_path}'ì— ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

    async with aiohttp.ClientSession(headers=DEFAULT_HEADERS) as session:
        scraper = LawScraper(oc_id=MY_OC_ID)

        async def run_collection(params, is_test=False):
            """ë‹¨ì¼ ìˆ˜ì§‘ ì‘ì—…ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
            filename_suffix = generate_filename_suffix(**params)
            period_output_dir = os.path.join(BASE_OUTPUT_DIR, filename_suffix)
            os.makedirs(period_output_dir, exist_ok=True)
            
            print(f"\n" + "="*50 + f"\nâ–¶ï¸ {'í…ŒìŠ¤íŠ¸' if is_test else 'ì „ì²´'} ìˆ˜ì§‘ ì‹¤í–‰: {filename_suffix}\n" + "="*50)
            
            list_params = {'display': 5 if is_test else 100, 'max_pages': 1 if is_test else None}
            df_list = await scraper.fetch_law_list(session, **params, **list_params)

            if df_list is None or df_list.empty:
                print(f"\nâš ï¸ {filename_suffix} ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None

            print("\n--- ì‹œí–‰ì¼ìë³„ ë²•ë ¹ ìˆ˜ ---")
            print(df_list['ì‹œí–‰ì¼ì'].value_counts())
            print("-------------------------\n")

            print(f"[ê²€ì¦] ê¸°ì¡´ì— ìˆ˜ì§‘ëœ íŒŒì¼ì„ í™•ì¸í•˜ì—¬ ëˆ„ë½ëœ í•­ëª©ë§Œ ì„ ë³„í•©ë‹ˆë‹¤...")
            rows_to_collect = []
            rows_to_skip = []
            
            for _, row in df_list.iterrows():
                base_filename = f"{row['ë²•ë ¹ID']}_{scraper._sanitize_filename(row['ë²•ë ¹ëª…í•œê¸€'])}"
                txt_path = os.path.join(period_output_dir, f"{base_filename}.txt")
                json_path = os.path.join(period_output_dir, f"{base_filename}.json")
                if await aio_os.path.exists(txt_path) and await aio_os.path.exists(json_path):
                    rows_to_skip.append(row)
                else:
                    rows_to_collect.append(row)
            
            status_map = {'SKIPPED_EXISTS': [dict(row) for row in rows_to_skip]}
            df_final_collected = pd.DataFrame()

            if rows_to_collect:
                df_to_collect = pd.DataFrame(rows_to_collect).reset_index(drop=True)
                df_final_collected, collected_status_map = await scraper._process_laws_batch(session, df_to_collect, period_output_dir)
                for key, value in collected_status_map.items():
                    status_map.setdefault(key, []).extend(value)
            else:
                print(" âœ… ëª¨ë“  í•­ëª©ì´ ì´ë¯¸ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹ ê·œ ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

            await scraper._write_period_summary(period_output_dir, filename_suffix, status_map)

            print(f"\n[í†µí•©] ê¸°ì¡´ íŒŒì¼ê³¼ ìƒˆë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ í†µí•©í•©ë‹ˆë‹¤...")
            skipped_texts = []
            for row_dict in status_map['SKIPPED_EXISTS']:
                base_filename = f"{row_dict['ë²•ë ¹ID']}_{scraper._sanitize_filename(row_dict['ë²•ë ¹ëª…í•œê¸€'])}"
                txt_path = os.path.join(period_output_dir, f"{base_filename}.txt")
                try:
                    async with aiofiles.open(txt_path, 'r', encoding='utf-8') as f:
                        text = await f.read()
                    skipped_texts.append(text)
                except Exception:
                    skipped_texts.append(None)
            
            df_skipped = pd.DataFrame(status_map['SKIPPED_EXISTS'])
            if not df_skipped.empty:
                df_skipped['ë²•ë ¹ë³¸ë¬¸'] = skipped_texts
            
            df_final_total = pd.concat([df_final_collected, df_skipped], ignore_index=True)
            if not df_final_total.empty:
                df_final_total = df_final_total.set_index('ë²•ë ¹ID').loc[df_list['ë²•ë ¹ID'].astype(str)].reset_index()
                json_path = os.path.join(period_output_dir, f"collected_laws_{filename_suffix}.json")
                df_final_total.to_json(json_path, orient='records', force_ascii=False, indent=4)
                print(f"\nğŸ’¾ ìµœì¢… í†µí•© ê²°ê³¼(JSON)ê°€ '{json_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return status_map

        # --- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ (ìµœì‹  5ê±´ ìƒ˜í”Œ ìˆ˜ì§‘) ---
        # print("\n" + "="*60 + "\nâ–¶ï¸ í…ŒìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤í–‰ (ìµœì‹  5ê±´)\n" + "="*60)
        # test_end_date = pd.to_datetime("2025-07-11")
        # test_start_date = test_end_date - pd.DateOffset(months=1)
        # test_range_str = f"{test_start_date.strftime('%Y%m%d')}~{test_end_date.strftime('%Y%m%d')}"
        # await run_collection({"efyd_range": test_range_str}, is_test=True) 
        # print("\n" + "="*60 + "\nâœ… í…ŒìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ. ì „ì²´ ìˆ˜ì§‘ì„ ì‹œì‘í•˜ë ¤ë©´ ì´ ë¶€ë¶„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.\n" + "="*60)


        # --- ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ (2025.06.01 ~ 2025.06.05) ---
        # ìœ„ í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ëë‚˜ë©´, ìœ„ í…ŒìŠ¤íŠ¸ ë¸”ë¡ì„ ì£¼ì„ ì²˜ë¦¬í•˜ê³  ì•„ë˜ ë¸”ë¡ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì „ì²´ ìˆ˜ì§‘ì„ ì‹œì‘í•˜ì„¸ìš”.
        
        start_date = pd.to_datetime("2025-06-01") 
        end_date = pd.to_datetime("2025-06-30")   
        print("\n" + "="*60 + f"\nâ–¶ï¸ ì‹¤ì œ ë²•ë ¹ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ ({start_date.date()} ~ {end_date.date()})\n" + "="*60)
        
        overall_stats = []
        collection_max_retries = 3

        date_ranges = pd.date_range(start=start_date, end=end_date, freq='MS') # MS: Month Start

        for i in range(len(date_ranges)):
            range_start = date_ranges[i]
            range_end = range_start + pd.offsets.MonthEnd(1)
            if range_end > end_date:
                range_end = end_date
            
            date_range_str = f"{range_start.strftime('%Y%m%d')}~{range_end.strftime('%Y%m%d')}"
            
            for attempt in range(collection_max_retries):
                try:
                    status_map = await run_collection({"efyd_range": date_range_str}) 
                    if status_map is not None:
                        overall_stats.append({'period': date_range_str, 'status': 'SUCCESS', 'stats': status_map})
                    else:
                        overall_stats.append({'period': date_range_str, 'status': 'NO_DATA', 'stats': {}})
                    break 
                except Exception as e:
                    print(f"â€¼ï¸ {date_range_str} ê¸°ê°„ ìˆ˜ì§‘ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt + 1}/{collection_max_retries}): {e}")
                    if attempt < collection_max_retries - 1:
                        await asyncio.sleep(5)
                    else:
                        print(f"â€¼ï¸ {date_range_str} ê¸°ê°„ ìˆ˜ì§‘ì— ìµœì¢… ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        overall_stats.append({'period': date_range_str, 'status': 'FATAL_ERROR', 'stats': {}})
            
            await _write_overall_summary(overall_stats)
        


if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
